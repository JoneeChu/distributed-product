### 产品介绍

1. 分布式同步训练
   1. API使用 `DistributeTranspiler` 将单机网络配置转换成可以多机执行的 `pserver` 端程序和 `trainer` 端程序。用户在不同的节点执行相同的一段代码，根据环境变量或启动参数， 可以执行对应的 `pserver` 或 `trainer` 角色。Fluid分布式同步训练同时支持pserver模式和NCCL2模式（paddle fluid是百度开源的深度学习框架paddlepaddle的最新版本）
2. 分布式异步训练
   1. API使用 `DistributeTranspiler` 将单机网络配置转换成可以多机执行的 `pserver` 端程序和 `trainer` 端程序。用户在不同的节点执行相同的一段代码，根据环境变量或启动参数， 可以执行对应的 `pserver` 或 `trainer` 角色。Fluid异步训练只支持pserver模式，异步训练和 [同步训练](https://www.paddlepaddle.org.cn/documentation/docs/zh/api_guides/low_level/distributed/sync_training.html) 的主要差异在于：异步训练每个trainer的梯度是单独更新到参数上的， 而同步训练是所有trainer的梯度合并之后统一更新到参数上，因此，同步训练和异步训练的超参数需要分别调节



### 个人总结

主要提供paddle fluid这一框架来支持模型的分布式训练，并未产品化